import os
import httpx 
import json
import re     
import html   
from sqlalchemy.orm import Session
from app.config import GMS_KEY 

# --- (ìœ ì§€) LCEL ì„í¬íŠ¸ ---
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from fastapi import HTTPException
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

# --- ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"
CHROMA_PERSIST_DIRECTORY = "./chroma_db" 

# 1. ëª¨ë¸ ë° ë²¡í„° ìŠ¤í† ì–´ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-large", 
        api_key=GMS_KEY,
        base_url=GMS_BASE_URL
    )
    
    llm = ChatOpenAI(
        temperature=0.7, 
        model_name="gpt-5", # (GMSì—ì„œ ì§€ì›í•˜ëŠ” gpt-5 ëª¨ë¸)
        api_key=GMS_KEY,
        base_url=GMS_BASE_URL
    )
except Exception as e:
    print(f"GMS/OpenAI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    embedding_model = None
    llm = None

# --- (ìœ ì§€) ID-ì»¬ë ‰ì…˜ëª… ë³€í™˜ í—¬í¼ í•¨ìˆ˜ ---
def _get_collection_name(document_id: str) -> str:
    if not document_id:
        raise ValueError("Document IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    return f"material_{document_id}"

# --- (ìœ ì§€) ì›Œí¬í”Œë¡œìš° 1: ì„ë² ë”© ìƒì„± (Service Logic) ---
# (download_json_from_cloudfront, _clean_html_content, 
#  extract_data_from_json, create_and_store_embeddings í•¨ìˆ˜ëŠ”
#  ì´ì „ê³¼ ë™ì¼í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµí•©ë‹ˆë‹¤.)

async def download_json_from_cloudfront(url: str) -> dict:
    # ... (ì´ì „ê³¼ ë™ì¼) ...
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, follow_redirects=True)
            if response.status_code != 200:
                raise HTTPException(status_code=response.status_code, 
                                    detail=f"CloudFront/S3 JSON ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (URL: {url}): HTTP {response.status_code}")
            return response.json()
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="JSON ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
    except json.JSONDecodeError:
        raise HTTPException(status_code=500, detail="ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"JSON ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def _clean_html_content(html_text: str) -> str:
    # ... (ì´ì „ê³¼ ë™ì¼) ...
    if not html_text:
        return ""
    text = re.sub(r'<br\s*/?>', '\n', html_text, flags=re.IGNORECASE)
    text = re.sub(r'<[^>]+>', ' ', text)
    text = html.unescape(text)
    text = ' '.join(text.split())
    return text

def extract_data_from_json(json_data: dict) -> list[Document]:
    # ... (ì´ì „ê³¼ ë™ì¼) ...
    documents = []
    chapters = json_data.get("chapters", [])
    if not chapters:
        raise ValueError("JSONì—ì„œ 'chapters' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    for chapter in chapters:
        chapter_id = chapter.get("id")
        title = chapter.get("title")
        content_html = chapter.get("content", "")
        chapter_type = chapter.get("type")
        base_metadata = {
            "chapter_id": chapter_id,
            "title": title,
            "type": chapter_type
        }
        if "ìƒˆ ì±•í„°ì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”" in content_html:
            print(f"Skipping empty chapter: {title}")
            continue
        if chapter_type == "content":
            plain_text = _clean_html_content(content_html)
            if plain_text.strip():
                documents.append(Document(
                    page_content=plain_text,
                    metadata=base_metadata
                ))
        elif chapter_type == "quiz":
            qa_list = chapter.get("qa", [])
            for qa_pair in qa_list:
                q = qa_pair.get("question", "")
                a = qa_pair.get("answer", "")
                qa_content = f"ì§ˆë¬¸: {q}\nì •ë‹µ: {a}"
                documents.append(Document(
                    page_content=qa_content,
                    metadata=base_metadata.copy() 
                ))
    print(f"JSON íŒŒì‹± ì™„ë£Œ. ì´ {len(documents)}ê°œì˜ Document ìƒì„±.")
    return documents

def create_and_store_embeddings(document_id: str, documents: list[Document]):
    # ... (ì´ì „ê³¼ ë™ì¼) ...
    if not documents:
        raise ValueError("ì„ë² ë”©í•  Documentê°€ ì—†ìŠµë‹ˆë‹¤.")
    if not embedding_model:
        raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(documents)
    if not chunks:
        print("ê²½ê³ : í…ìŠ¤íŠ¸ ë¶„í•  í›„ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    collection_name = _get_collection_name(document_id)
    print(f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ. ì´ {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±. ì»¬ë ‰ì…˜: {collection_name}")
    vector_store = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        collection_name=collection_name, 
        persist_directory=CHROMA_PERSIST_DIRECTORY
    )
    print(f"'{document_id}' (ì»¬ë ‰ì…˜: {collection_name}) ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ.")


# --- (ìˆ˜ì •) ì›Œí¬í”Œë¡œìš° 2: RAG ì§ˆì˜ì‘ë‹µ (TTS/ê°„ê²°í•œ ë‹µë³€ ìµœì í™”) ---

def get_rag_chain(document_id: str): 
    """
    (ìˆ˜ì •) LCEL ì²´ì¸ + TTS/ê°„ê²°í•œ ë‹µë³€ í”„ë¡¬í”„íŠ¸ ì ìš©
    """
    if not embedding_model or not llm:
        raise ValueError("LLM ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    collection_name = _get_collection_name(document_id)

    # 1. Retriever ë¡œë“œ
    # (ğŸš¨ ìˆ˜ì • 1) LLM ì…ë ¥(Context) ìµœì†Œí™”ë¡œ ì‘ë‹µ ì†ë„ í–¥ìƒ
    # k=5 (ë„ˆë¬´ ë§ìŒ) -> k=3 (ì ì ˆ)
    retriever = Chroma(
        persist_directory=CHROMA_PERSIST_DIRECTORY,
        embedding_function=embedding_model,
        collection_name=collection_name 
    ).as_retriever(
        search_type="mmr",
        search_kwargs={"k": 3} # 5 -> 3
    )

    # 2. (ìœ ì§€) 1ë‹¨ê³„: ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
    rephrase_prompt = ChatPromptTemplate.from_messages([
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        ("user", "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ìœ„ ì§ˆë¬¸ì„ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
    ])
    
    # 3. (ìœ ì§€) 1ë‹¨ê³„ ì²´ì¸: History-Aware Retriever
    history_aware_retriever = create_history_aware_retriever(
        llm=llm, 
        retriever=retriever, 
        prompt=rephrase_prompt
    )

    # 4. (ğŸš¨ ìˆ˜ì • 2) 2ë‹¨ê³„: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (TTS ìµœì í™”)
    # LLMì´ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ë„ë¡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •
    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """
         ë‹¹ì‹ ì€ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ AI êµì‚¬ì…ë‹ˆë‹¤.
         
         [!!ì¤‘ìš” ê·œì¹™!!]
         1. í•™ìƒì€ ì´ ë‹µë³€ì„ ëª¨ë°”ì¼ì—ì„œ **ìŒì„±(TTS)ìœ¼ë¡œ ë“£ìŠµë‹ˆë‹¤.**
         2. ë”°ë¼ì„œ, ë‹µë³€ì€ **ë°˜ë“œì‹œ 1~2ë¬¸ì¥ì˜ ê°„ê²°í•˜ê³  ëª…í™•í•œ í•µì‹¬ ìš”ì•½**ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
         3. ì ˆëŒ€ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”. í•™ìƒì´ ë“£ê¸°ì— ë¶ˆí¸í•©ë‹ˆë‹¤.
         
         [ë‹µë³€ ìƒì„± ê·œì¹™]
         1. ì˜¤ì§ ì•„ë˜ì— ì œê³µë˜ëŠ” [ë¬¸ì„œ ë‚´ìš©]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
         2. ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ìë£Œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
         3. ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ ë¬¸ì„œì˜ [ì¶œì²˜]ë¥¼ (ì¶œì²˜: ...) í˜•ì‹ìœ¼ë¡œ ë§ë¶™ì—¬ì£¼ì„¸ìš”.
         
         [ë¬¸ì„œ ë‚´ìš©]:
         {context}
         """),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}")
    ])
    
    # 5. (ìœ ì§€) 2ë‹¨ê³„ ì²´ì¸: Document Chain
    document_chain = create_stuff_documents_chain(llm, answer_prompt)
    
    # 6. (ìœ ì§€) 1ë‹¨ê³„ì™€ 2ë‹¨ê³„ë¥¼ ê²°í•©í•œ ìµœì¢… ì²´ì¸
    conversational_retrieval_chain = create_retrieval_chain(
        history_aware_retriever, # 1ë‹¨ê³„
        document_chain         # 2ë‹¨ê³„
    )
    
    return conversational_retrieval_chain