import os
import httpx
import json
import re
import html
from typing import List
from sqlalchemy.orm import Session
from app.config import GMS_KEY
from app.config import HUGGINGFACE_TOKEN

# --- LCEL ì„í¬íŠ¸ ---
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from fastapi import HTTPException
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.chains import (
    create_history_aware_retriever,
    create_retrieval_chain,
)
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

# --- Re-ranking ì„í¬íŠ¸ ---
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder


# --- ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"
CHROMA_PERSIST_DIRECTORY = "./chroma_db"

# 1. ëª¨ë¸ ë° ë²¡í„° ìŠ¤í† ì–´ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-large", api_key=GMS_KEY, base_url=GMS_BASE_URL
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")

    llm = ChatOpenAI(
        temperature=0.7, model_name="gpt-5", api_key=GMS_KEY, base_url=GMS_BASE_URL
    )
    print("âœ… LLM ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")

    # --- Reranker ëª¨ë¸ ì´ˆê¸°í™” (ë‹¤ì¤‘ fallback ì „ëµ) ---
    reranker_model = None

    # ì‹œë„ 1: í•œêµ­ì–´ ìµœì í™” ëª¨ë¸ (í† í° í•„ìš”)
    if HUGGINGFACE_TOKEN:
        try:
            reranker_model = HuggingFaceCrossEncoder(
                model_name="Dongjin-kr/ko-reranker",
                model_kwargs={
                    'device': 'cpu',
                    'trust_remote_code': True,
                    'token': HUGGINGFACE_TOKEN  # âœ… ìˆ˜ì •: use_auth_token -> token
                }
            )
            print("âœ… Reranker ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (Dongjin-kr/ko-reranker)")
        except Exception as e:
            print(f"âš ï¸ í•œêµ­ì–´ Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì‹œë„ 2: ê³µê°œ ë‹¤êµ­ì–´ ëª¨ë¸ (í† í° ë¶ˆí•„ìš”)
    if reranker_model is None:
        try:
            reranker_model = HuggingFaceCrossEncoder(
                model_name="BAAI/bge-reranker-base",
                model_kwargs={'device': 'cpu'}
            )
            print("âœ… Reranker ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (BAAI/bge-reranker-base)")
        except Exception as e:
            print(f"âš ï¸ BAAI Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì‹œë„ 3: ê°€ì¥ ì•ˆì •ì ì¸ ì˜ì–´ ëª¨ë¸ (ìµœì¢… fallback)
    if reranker_model is None:
        try:
            reranker_model = HuggingFaceCrossEncoder(
                model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
                model_kwargs={'device': 'cpu'}
            )
            print("âœ… Reranker ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (ms-marco-MiniLM-L-6-v2)")
        except Exception as e:
            print(f"âŒ ëª¨ë“  Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            reranker_model = None

except Exception as e:
    print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    embedding_model = None
    llm = None
    reranker_model = None


# --- ID-ì»¬ë ‰ì…˜ëª… ë³€í™˜ í—¬í¼ í•¨ìˆ˜ ---
def _get_collection_name(document_id: str) -> str:
    """
    document_idë¥¼ Chroma ì»¬ë ‰ì…˜ëª…ìœ¼ë¡œ ë³€í™˜
    ì˜ˆ: '123abc' -> 'material_123abc'
    """
    if not document_id:
        raise ValueError("Document IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # Chroma ì»¬ë ‰ì…˜ëª… ê·œì¹™: ì•ŒíŒŒë²³, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´ë§Œ í—ˆìš©, 3-63ì
    sanitized_id = re.sub(r"[^a-zA-Z0-9_]", "_", document_id)
    collection_name = f"material_{sanitized_id}"

    if len(collection_name) > 63:
        collection_name = collection_name[:63]

    return collection_name


# --- ì›Œí¬í”Œë¡œìš° 1: ì„ë² ë”© ìƒì„± (Service Logic) ---


async def download_json_from_cloudfront(url: str) -> dict:
    """
    CloudFront/S3 URLì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, follow_redirects=True)

            if response.status_code != 200:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"CloudFront/S3 JSON ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (URL: {url}): HTTP {response.status_code}",
                )

            return response.json()

    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="JSON ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
    except json.JSONDecodeError:
        raise HTTPException(
            status_code=500, detail="ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤."
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"JSON ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


def _clean_html_content(html_text: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if not html_text:
        return ""

    # <br> íƒœê·¸ë¥¼ ê°œí–‰ ë¬¸ìë¡œ ë³€í™˜
    text = re.sub(r"<br\s*/?>", "\n", html_text, flags=re.IGNORECASE)

    # ëª¨ë“  HTML íƒœê·¸ ì œê±°
    text = re.sub(r"<[^>]+>", " ", text)

    # HTML ì—”í‹°í‹° ë””ì½”ë”© (&nbsp; -> ê³µë°± ë“±)
    text = html.unescape(text)

    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì••ì¶•
    text = " ".join(text.split())

    return text


def extract_data_from_json(json_data: dict) -> List[Document]:
    """
    JSON ë°ì´í„°ì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    documents = []
    chapters = json_data.get("chapters", [])

    if not chapters:
        raise ValueError(
            "JSONì—ì„œ 'chapters' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        )

    for chapter in chapters:
        chapter_id = chapter.get("id")
        title = chapter.get("title")
        content_html = chapter.get("content", "")
        chapter_type = chapter.get("type")

        base_metadata = {
            "chapter_id": str(chapter_id),  # ğŸ”§ ìˆ˜ì •: ë¬¸ìì—´ë¡œ ë³€í™˜
            "title": title or "ì œëª© ì—†ìŒ",  # ğŸ”§ ìˆ˜ì •: None ë°©ì§€
            "type": chapter_type,
        }

        # ë¹ˆ ì±•í„° ìŠ¤í‚µ
        if "ìƒˆ ì±•í„°ì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”" in content_html:
            print(f"ë¹ˆ ì±•í„° ê±´ë„ˆëœ€: {title}")
            continue

        # íƒ€ì…ë³„ ì²˜ë¦¬
        if chapter_type == "content":
            plain_text = _clean_html_content(content_html)
            if plain_text.strip():
                documents.append(
                    Document(page_content=plain_text, metadata=base_metadata)
                )

        elif chapter_type == "quiz":
            qa_list = chapter.get("qa", [])
            for idx, qa_pair in enumerate(qa_list):  # ğŸ”§ ìˆ˜ì •: ì¸ë±ìŠ¤ ì¶”ê°€
                q = qa_pair.get("question", "")
                a = qa_pair.get("answer", "")

                if not q or not a:  # ğŸ”§ ìˆ˜ì •: ë¹ˆ Q&A ìŠ¤í‚µ
                    continue

                qa_content = f"ì§ˆë¬¸: {q}\nì •ë‹µ: {a}"

                # ğŸ”§ ìˆ˜ì •: Q&A ë©”íƒ€ë°ì´í„°ì— ì¸ë±ìŠ¤ ì¶”ê°€
                qa_metadata = base_metadata.copy()
                qa_metadata["qa_index"] = idx

                documents.append(
                    Document(page_content=qa_content, metadata=qa_metadata)
                )

    print(f"JSON íŒŒì‹± ì™„ë£Œ. ì´ {len(documents)}ê°œì˜ Document ìƒì„±.")
    return documents


def create_and_store_embeddings(document_id: str, documents: List[Document]):
    """
    Document ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ Chroma DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not documents:
        raise ValueError("ì„ë² ë”©í•  Documentê°€ ì—†ìŠµë‹ˆë‹¤.")

    if not embedding_model:
        raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ğŸ”§ ìˆ˜ì •: íƒ€ì…ë³„ ì²­í¬ í¬ê¸° ìµœì í™”
    content_chunks = []
    quiz_chunks = []

    for doc in documents:
        if doc.metadata.get("type") == "quiz":
            # í€´ì¦ˆëŠ” ë¶„í• í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ìœ ì§€
            quiz_chunks.append(doc)
        else:
            # ì¼ë°˜ ì½˜í…ì¸ ë§Œ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, chunk_overlap=100
            )
            content_chunks.extend(text_splitter.split_documents([doc]))

    all_chunks = content_chunks + quiz_chunks

    if not all_chunks:
        print("âš ï¸ ê²½ê³ : í…ìŠ¤íŠ¸ ë¶„í•  í›„ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    collection_name = _get_collection_name(document_id)
    print(
        f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ. ì´ {len(all_chunks)}ê°œì˜ ì²­í¬ ìƒì„± "
        f"(ì½˜í…ì¸ : {len(content_chunks)}, í€´ì¦ˆ: {len(quiz_chunks)}). "
        f"ì»¬ë ‰ì…˜: {collection_name}"
    )

    # ğŸ”§ ìˆ˜ì •: ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„± (ì„ íƒì )
    try:
        existing_vectorstore = Chroma(
            persist_directory=CHROMA_PERSIST_DIRECTORY,
            embedding_function=embedding_model,
            collection_name=collection_name,
        )
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
        existing_vectorstore.delete_collection()
        print(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œë¨")
    except Exception:
        pass  # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ

    # Chroma DBì— ì €ì¥
    vector_store = Chroma.from_documents(
        documents=all_chunks,
        embedding=embedding_model,
        collection_name=collection_name,
        persist_directory=CHROMA_PERSIST_DIRECTORY,
    )

    print(f"âœ… '{document_id}' (ì»¬ë ‰ì…˜: {collection_name}) ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ.")


# --- ì›Œí¬í”Œë¡œìš° 2: RAG ì§ˆì˜ì‘ë‹µ (Re-ranking ì ìš©) ---


def get_rag_chain(document_id: str):
    """
    Re-rankingì´ ì ìš©ëœ LCEL ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë™ì‘ íë¦„:
    1. Base Retrieverë¡œ 10ê°œ ë¬¸ì„œ ê²€ìƒ‰ (MMR ë°©ì‹)
    2. Rerankerë¡œ ìƒìœ„ 3ê°œ ì¬ì •ë ¬
    3. History-Aware Retrieverë¡œ ì§ˆë¬¸ ì¬êµ¬ì„±
    4. ìµœì¢… ë‹µë³€ ìƒì„±
    """
    # ëª¨ë¸ ì´ˆê¸°í™” ì²´í¬
    if not embedding_model or not llm:
        raise ValueError("LLM ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    collection_name = _get_collection_name(document_id)

    # ğŸ”§ ìˆ˜ì •: ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    try:
        vectorstore = Chroma(
            persist_directory=CHROMA_PERSIST_DIRECTORY,
            embedding_function=embedding_model,
            collection_name=collection_name,
        )
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        vectorstore.similarity_search("test", k=1)
    except Exception as e:
        raise ValueError(f"'{document_id}' ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

    # 1ë‹¨ê³„: Base Retriever (ë„“ì€ ê²€ìƒ‰)
    base_retriever = vectorstore.as_retriever(
        search_type="mmr",  # Maximum Marginal Relevance (ë‹¤ì–‘ì„± ë³´ì¥)
        search_kwargs={
            "k": 10,  # ì´ˆê¸° ê²€ìƒ‰: 10ê°œ
            "fetch_k": 20,  # ğŸ”§ ìˆ˜ì •: MMR í›„ë³´ í’€ í™•ëŒ€
        },
    )

    # 2ë‹¨ê³„: Reranker ì ìš© (ì„ íƒì )
    if reranker_model:
        print(f"âœ… Reranker ì ìš©: 10ê°œ â†’ ìƒìœ„ 3ê°œ ì¬ì •ë ¬")

        compressor = CrossEncoderReranker(
            model=reranker_model, top_n=3  # ìµœì¢… 3ê°œë§Œ ì„ íƒ
        )

        final_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )
    else:
        print(f"âš ï¸ Reranker ë¯¸ì ìš©: Base Retrieverë§Œ ì‚¬ìš© (k=5ë¡œ ì¡°ì •)")
        # Reranker ì—†ì„ ê²½ìš° ê²€ìƒ‰ ìˆ˜ ì¡°ì •
        final_retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 5,  # Reranker ì—†ìœ¼ë©´ 5ê°œë§Œ ê²€ìƒ‰
                "fetch_k": 15,  # ğŸ”§ ìˆ˜ì •: fetch_k ì¶”ê°€
            },
        )

    # 3ë‹¨ê³„: ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ (ëŒ€í™” ë§¥ë½ ë°˜ì˜)
    rephrase_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
            (
                "user",
                "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ìœ„ ì§ˆë¬¸ì„ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
                "ì§ˆë¬¸ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.",  # ğŸ”§ ìˆ˜ì •: ëª…í™•í•œ ì§€ì‹œ
            ),
        ]
    )

    # 4ë‹¨ê³„: History-Aware Retriever
    history_aware_retriever = create_history_aware_retriever(
        llm=llm, retriever=final_retriever, prompt=rephrase_prompt
    )

    # 5ë‹¨ê³„: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (TTS ìµœì í™”)
    answer_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ AI êµì‚¬ì…ë‹ˆë‹¤.
                [!!ì¤‘ìš” ê·œì¹™!!]
                1. í•™ìƒì€ ì´ ë‹µë³€ì„ ëª¨ë°”ì¼ì—ì„œ **ìŒì„±(TTS)ìœ¼ë¡œ ë“£ìŠµë‹ˆë‹¤.**
                2. ë”°ë¼ì„œ, ë‹µë³€ì€ **ë°˜ë“œì‹œ 1~2ë¬¸ì¥ì˜ ê°„ê²°í•˜ê³  ëª…í™•í•œ í•µì‹¬ ìš”ì•½**ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
                3. ì ˆëŒ€ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”. í•™ìƒì´ ë“£ê¸°ì— ë¶ˆí¸í•©ë‹ˆë‹¤.
                
                [ë‹µë³€ ìƒì„± ê·œì¹™]
                1. ì˜¤ì§ ì•„ë˜ì— ì œê³µë˜ëŠ” [ë¬¸ì„œ ë‚´ìš©]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
                2. ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ìë£Œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
                3. ë‹µë³€ì—ëŠ” ì¶œì²˜ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. (TTSë¡œ ë“£ê¸° ë•Œë¬¸)
                
                [ë¬¸ì„œ ë‚´ìš©]:
                {context}""",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ]
    )

    # 6ë‹¨ê³„: Document Chain
    document_chain = create_stuff_documents_chain(llm, answer_prompt)

    # 7ë‹¨ê³„: ìµœì¢… Retrieval Chain
    conversational_retrieval_chain = create_retrieval_chain(
        history_aware_retriever, document_chain
    )

    return conversational_retrieval_chain
