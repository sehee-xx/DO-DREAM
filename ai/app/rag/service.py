import os
import httpx
import json
import re
import html
from typing import List
from sqlalchemy.orm import Session
from app.config import GMS_KEY
from app.config import HUGGINGFACE_TOKEN

# --- LCEL ì„í¬íŠ¸ ---
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from fastapi import HTTPException
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_classic.chains import (
    create_history_aware_retriever,
    create_retrieval_chain,
)
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

# --- Re-ranking ì„í¬íŠ¸ ---
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder


# --- ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™” ---
GMS_BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"
CHROMA_PERSIST_DIRECTORY = "./chroma_db"

# 1. ëª¨ë¸ ë° ë²¡í„° ìŠ¤í† ì–´ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-3-large", api_key=GMS_KEY, base_url=GMS_BASE_URL
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")

    llm = ChatOpenAI(
        temperature=0.7, model_name="gpt-5-mini", api_key=GMS_KEY, base_url=GMS_BASE_URL
    )
    print("âœ… LLM ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ")

    # --- Reranker ëª¨ë¸ ì´ˆê¸°í™” (ë‹¤ì¤‘ fallback ì „ëµ) ---
    reranker_model = None

    # ì‹œë„ 1: í•œêµ­ì–´ ìµœì í™” ëª¨ë¸ (í† í° í•„ìš”)
    if HUGGINGFACE_TOKEN:
        try:
            reranker_model = HuggingFaceCrossEncoder(
                model_name="Dongjin-kr/ko-reranker",
                model_kwargs={
                    'device': 'cpu',
                    'trust_remote_code': True,
                    'token': HUGGINGFACE_TOKEN
                }
            )
            print("âœ… Reranker ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (Dongjin-kr/ko-reranker)")
        except Exception as e:
            print(f"âš ï¸ í•œêµ­ì–´ Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì‹œë„ 2: ê³µê°œ ë‹¤êµ­ì–´ ëª¨ë¸ (í† í° ë¶ˆí•„ìš”)
    if reranker_model is None:
        try:
            reranker_model = HuggingFaceCrossEncoder(
                model_name="BAAI/bge-reranker-base",
                model_kwargs={'device': 'cpu'}
            )
            print("âœ… Reranker ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (BAAI/bge-reranker-base)")
        except Exception as e:
            print(f"âš ï¸ BAAI Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì‹œë„ 3: ê°€ì¥ ì•ˆì •ì ì¸ ì˜ì–´ ëª¨ë¸ (ìµœì¢… fallback)
    if reranker_model is None:
        try:
            reranker_model = HuggingFaceCrossEncoder(
                model_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
                model_kwargs={'device': 'cpu'}
            )
            print("âœ… Reranker ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (ms-marco-MiniLM-L-6-v2)")
        except Exception as e:
            print(f"âŒ ëª¨ë“  Reranker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            reranker_model = None

except Exception as e:
    print(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    embedding_model = None
    llm = None
    reranker_model = None


# --- ID-ì»¬ë ‰ì…˜ëª… ë³€í™˜ í—¬í¼ í•¨ìˆ˜ ---
def _get_collection_name(document_id: str) -> str:
    """
    document_idë¥¼ Chroma ì»¬ë ‰ì…˜ëª…ìœ¼ë¡œ ë³€í™˜
    
    [ìˆ˜ì •ëœ ê·œì¹™]
    í•­ìƒ 'material_' ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ì„œ ChromaDBì˜ Naming Rule(3ì ì´ìƒ)ì„ ë§Œì¡±ì‹œí‚¤ê³ ,
    ì…ë ¥ëœ document_idì— ë”°ë¼ ë¶„ê¸°ë©ë‹ˆë‹¤.
    
    - ì…ë ¥ "14" -> "material_14" (ìµœì¢…ë³¸)
    - ì…ë ¥ "pdf_14" -> "material_pdf_14" (ì´ˆê¸°ë³¸)
    """
    if not document_id:
        raise ValueError("Document IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜
    sanitized_id = re.sub(r"[^a-zA-Z0-9_]", "_", document_id)

    # ğŸ”§ ìˆ˜ì •: ì¡°ê±´ë¬¸ ì œê±°í•˜ê³  í•­ìƒ material_ ì ‘ë‘ì‚¬ ì‚¬ìš©
    collection_name = f"material_{sanitized_id}"

    # Chroma ì»¬ë ‰ì…˜ëª… ê¸¸ì´ ì œí•œ (63ì)
    if len(collection_name) > 63:
        collection_name = collection_name[:63]

    return collection_name


# --- ì›Œí¬í”Œë¡œìš° 1: ì„ë² ë”© ìƒì„± (Service Logic) ---

async def download_json_from_cloudfront(url: str) -> dict:
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(url, follow_redirects=True)
            if response.status_code != 200:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"CloudFront/S3 JSON ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (URL: {url}): HTTP {response.status_code}",
                )
            return response.json()
    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="JSON ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼")
    except json.JSONDecodeError:
        raise HTTPException(
            status_code=500, detail="ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìœ íš¨í•œ JSONì´ ì•„ë‹™ë‹ˆë‹¤."
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"JSON ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


def _clean_html_content(html_text: str) -> str:
    if not html_text:
        return ""
    text = re.sub(r"<br\s*/?>", "\n", html_text, flags=re.IGNORECASE)
    text = re.sub(r"<[^>]+>", " ", text)
    text = html.unescape(text)
    text = " ".join(text.split())
    return text


# --- ë©”ì¸ ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ (chapters ìŠ¤í‚¤ë§ˆ) ---
def extract_data_from_json(json_data: dict) -> List[Document]:
    """
    JSON ë°ì´í„°('chapters' ìŠ¤í‚¤ë§ˆ)ì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    documents = []
    chapters = json_data.get("chapters", [])

    if not chapters:
        return []

    print(f"ğŸ“– 'chapters' ìŠ¤í‚¤ë§ˆ ê°ì§€: ì´ {len(chapters)}ê°œ ì±•í„° ì²˜ë¦¬ ì‹œì‘")

    for chapter in chapters:
        chapter_id = chapter.get("id")
        title = chapter.get("title")
        content_html = chapter.get("content", "")
        chapter_type = chapter.get("type")

        base_metadata = {
            "chapter_id": str(chapter_id),
            "title": title or "ì œëª© ì—†ìŒ",
            "type": chapter_type,
        }

        if "ìƒˆ ì±•í„°ì˜ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”" in content_html:
            continue

        if chapter_type == "content":
            plain_text = _clean_html_content(content_html)
            if plain_text.strip():
                documents.append(
                    Document(page_content=plain_text, metadata=base_metadata)
                )

        elif chapter_type == "quiz":
            qa_list = chapter.get("qa", [])
            for idx, qa_pair in enumerate(qa_list):
                q = qa_pair.get("question", "")
                a = qa_pair.get("answer", "")
                if not q or not a:
                    continue
                qa_content = f"ì§ˆë¬¸: {q}\nì •ë‹µ: {a}"
                qa_metadata = base_metadata.copy()
                qa_metadata["qa_index"] = idx
                documents.append(
                    Document(page_content=qa_content, metadata=qa_metadata)
                )

    print(f"âœ… JSON íŒŒì‹± ì™„ë£Œ. ì´ {len(documents)}ê°œì˜ Document ìƒì„±.")
    return documents


# --- ì´ˆê¸° ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›) ---
def extract_initial_data_from_json(json_data: dict) -> List[Document]:
    """
    í…ìŠ¤íŠ¸ ì¶”ì¶œ ì§í›„ JSONì—ì„œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    'chapters' ìŠ¤í‚¤ë§ˆì™€ 'parsedData' ìŠ¤í‚¤ë§ˆ(Flat ë° Nested)ë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    
    # 1. 'chapters' í‚¤ í™•ì¸ (ìš°ì„  ìˆœìœ„: ì‹ ê·œ ìŠ¤í‚¤ë§ˆ)
    if "chapters" in json_data:
        print("â„¹ï¸ [ì´ˆê¸° ì„ë² ë”©] 'chapters' ìŠ¤í‚¤ë§ˆê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return extract_data_from_json(json_data)

    # 2. 'parsedData' ì²˜ë¦¬ (êµ¬í˜• ìŠ¤í‚¤ë§ˆ)
    data_list = []
    
    # Case A: parsedData ë˜í¼ê°€ ìˆëŠ” ê²½ìš°
    if "parsedData" in json_data:
        print("â„¹ï¸ [ì´ˆê¸° ì„ë² ë”©] 'parsedData' Wrapper ê°ì§€")
        data_list = json_data["parsedData"].get("data", [])
    # Case B: parsedData ì—†ì´ ë°”ë¡œ dataê°€ ìˆëŠ” ê²½ìš° (S3 ì €ì¥ ë°©ì‹)
    elif "data" in json_data:
        print("â„¹ï¸ [ì´ˆê¸° ì„ë² ë”©] Flat 'data' êµ¬ì¡° ê°ì§€")
        data_list = json_data.get("data", [])
    
    if not data_list:
        keys = list(json_data.keys())
        raise ValueError(f"JSONì—ì„œ ìœ íš¨í•œ ë°ì´í„°('chapters', 'parsedData', 'data')ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Keys: {keys})")
    
    print(f"â„¹ï¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ í™•ì¸ë¨ ({len(data_list)}ê°œ ì¸ë±ìŠ¤)")
    
    documents = []
    
    for data_item in data_list:
        index = data_item.get("index", "unknown")
        index_title = data_item.get("index_title", "ì œëª© ì—†ìŒ")
        titles = data_item.get("titles", [])
        
        for title_item in titles:
            title = title_item.get("title", "")
            if "ê°œë…" in title and ("check" in title.lower() or "Check" in title):
                continue
            
            s_titles = title_item.get("s_titles", [])
            for s_title_item in s_titles:
                s_title = s_title_item.get("s_title", "")
                contents = s_title_item.get("contents")
                
                base_metadata = {
                    "index": index,
                    "index_title": index_title,
                    "title": title,
                    "s_title": s_title,
                    "type": "content"
                }
                
                if contents and contents.strip():
                    full_text = f"{title}\n{s_title}\n{contents}"
                    documents.append(Document(page_content=full_text, metadata=base_metadata))
                
                ss_titles = s_title_item.get("ss_titles", [])
                for ss_title_item in ss_titles:
                    ss_title = ss_title_item.get("ss_title", "")
                    ss_contents = ss_title_item.get("contents")
                    
                    if ss_contents and ss_contents.strip():
                        ss_metadata = base_metadata.copy()
                        ss_metadata["ss_title"] = ss_title
                        full_text = f"{title}\n{s_title}\n{ss_title}\n{ss_contents}"
                        documents.append(Document(page_content=full_text, metadata=ss_metadata))
    
    print(f"âœ… ì´ˆê¸° íŒŒì‹± ì™„ë£Œ. ì´ {len(documents)}ê°œì˜ Document ìƒì„±.")
    return documents


def create_and_store_embeddings(document_id: str, documents: List[Document]):
    """
    Document ë¦¬ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ Chroma DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not documents:
        raise ValueError("ì„ë² ë”©í•  Documentê°€ ì—†ìŠµë‹ˆë‹¤.")

    if not embedding_model:
        raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # íƒ€ì…ë³„ ì²­í¬ í¬ê¸° ìµœì í™”
    content_chunks = []
    quiz_chunks = []

    for doc in documents:
        if doc.metadata.get("type") == "quiz":
            quiz_chunks.append(doc)
        else:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, chunk_overlap=100
            )
            content_chunks.extend(text_splitter.split_documents([doc]))

    all_chunks = content_chunks + quiz_chunks

    if not all_chunks:
        print("âš ï¸ ê²½ê³ : í…ìŠ¤íŠ¸ ë¶„í•  í›„ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    collection_name = _get_collection_name(document_id)
    print(
        f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ. ì´ {len(all_chunks)}ê°œì˜ ì²­í¬ ìƒì„± "
        f"(ì½˜í…ì¸ : {len(content_chunks)}, í€´ì¦ˆ: {len(quiz_chunks)}). "
        f"ì»¬ë ‰ì…˜: {collection_name}"
    )

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ í›„ ì¬ìƒì„±
    try:
        existing_vectorstore = Chroma(
            persist_directory=CHROMA_PERSIST_DIRECTORY,
            embedding_function=embedding_model,
            collection_name=collection_name,
        )
        existing_vectorstore.delete_collection()
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œë¨ (ì¬ìƒì„±)")
    except Exception:
        pass 

    # Chroma DBì— ì €ì¥
    vector_store = Chroma.from_documents(
        documents=all_chunks,
        embedding=embedding_model,
        collection_name=collection_name,
        persist_directory=CHROMA_PERSIST_DIRECTORY,
    )

    print(f"âœ… '{document_id}' (ì»¬ë ‰ì…˜: {collection_name}) ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ.")


# --- ì´ˆê¸° ì„ë² ë”© ì „ìš© í•¨ìˆ˜ (ë‹¨ìˆœ ë˜í¼) ---
def create_initial_embeddings(pdf_id: str, documents: List[Document]):
    """
    ì´ˆê¸° ì„ë² ë”© ìƒì„±
    document_id ì•ì— 'pdf_' ì ‘ë‘ì‚¬ë¥¼ ë¶™ì—¬ì„œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # ğŸ”§ ìˆ˜ì •: pdf_id ì•ì— 'pdf_'ë¥¼ ë¶™ì—¬ì„œ ì»¬ë ‰ì…˜ëª… êµ¬ë¶„
    # ì˜ˆ: ì…ë ¥ '14' -> 'pdf_14' -> _get_collection_name -> 'material_pdf_14'
    prefixed_id = f"pdf_{pdf_id}"
    create_and_store_embeddings(prefixed_id, documents)


# --- ì›Œí¬í”Œë¡œìš° 2: RAG ì§ˆì˜ì‘ë‹µ (Re-ranking ì ìš©) ---
# (get_rag_chain í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)

def get_rag_chain(document_id: str):
    if not embedding_model or not llm:
        raise ValueError("LLM ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    collection_name = _get_collection_name(document_id)
    print(f"ğŸ”— RAG Chain ì—°ê²°: {collection_name}")

    try:
        vectorstore = Chroma(
            persist_directory=CHROMA_PERSIST_DIRECTORY,
            embedding_function=embedding_model,
            collection_name=collection_name,
        )
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ìš© ì¿¼ë¦¬
        vectorstore.similarity_search("test", k=1)
    except Exception as e:
        raise ValueError(f"'{collection_name}' ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ID: {document_id}): {e}")

    base_retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 10, "fetch_k": 20},
    )

    if reranker_model:
        print(f"âœ… Reranker ì ìš©: 10ê°œ â†’ ìƒìœ„ 3ê°œ ì¬ì •ë ¬")
        compressor = CrossEncoderReranker(
            model=reranker_model, top_n=3
        )
        final_retriever = ContextualCompressionRetriever(
            base_compressor=compressor, base_retriever=base_retriever
        )
    else:
        print(f"âš ï¸ Reranker ë¯¸ì ìš©: Base Retrieverë§Œ ì‚¬ìš© (k=5ë¡œ ì¡°ì •)")
        final_retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 5, "fetch_k": 15},
        )

    rephrase_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
            (
                "user",
                "ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ìœ„ ì§ˆë¬¸ì„ ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”. "
                "ì§ˆë¬¸ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.",
            ),
        ]
    )

    history_aware_retriever = create_history_aware_retriever(
        llm=llm, retriever=final_retriever, prompt=rephrase_prompt
    )

    answer_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¹ì‹ ì€ í•™ìƒì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ AI êµì‚¬ì…ë‹ˆë‹¤.
                [!!ì¤‘ìš” ê·œì¹™!!]
                1. í•™ìƒì€ ì´ ë‹µë³€ì„ ëª¨ë°”ì¼ì—ì„œ **ìŒì„±(TTS)ìœ¼ë¡œ ë“£ìŠµë‹ˆë‹¤.**
                2. ë”°ë¼ì„œ, ë‹µë³€ì€ **ë°˜ë“œì‹œ 1~2ë¬¸ì¥ì˜ ê°„ê²°í•˜ê³  ëª…í™•í•œ í•µì‹¬ ìš”ì•½**ìœ¼ë¡œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
                3. ì ˆëŒ€ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”. í•™ìƒì´ ë“£ê¸°ì— ë¶ˆí¸í•©ë‹ˆë‹¤.
                
                [ë‹µë³€ ìƒì„± ê·œì¹™]
                1. ì˜¤ì§ ì•„ë˜ì— ì œê³µë˜ëŠ” [ë¬¸ì„œ ë‚´ìš©]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
                2. ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ìë£Œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
                3. ë‹µë³€ì—ëŠ” ì¶œì²˜ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. (TTSë¡œ ë“£ê¸° ë•Œë¬¸)
                
                [ë¬¸ì„œ ë‚´ìš©]:
                {context}""",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("user", "{input}"),
        ]
    )

    document_chain = create_stuff_documents_chain(llm, answer_prompt)

    conversational_retrieval_chain = create_retrieval_chain(
        history_aware_retriever, document_chain
    )

    return conversational_retrieval_chain
